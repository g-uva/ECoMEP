# üå±üåç‚ôªÔ∏è ECoMEP - Edge-Cloud Continuum Multi-Level ML Energy Prediction Framework (GreenDIGIT project)

>**Disclaimer**: the information on this README is still temporary. The tools, architecture and other specifications are subject to change.

> Part of GreenDIGIT WP6.2 ‚Äî Predictive AI for Federated Energy-Aware Workflows  
> Developed in collaboration with SoBigData RI, IFCA, DIRAC, and GreenDIGIT RIs and partners.


## Overview

This framework enables **real-time predictive modelling** across **Edge‚ÄìFog‚ÄìCloud infrastructures** using **multi-level machine learning pipelines**. It ingests environmental and performance metrics (e.g. energy, CPU usage, workload profiles) from **distributed clusters and IoT devices**, processes them, and trains models to **forecast resource usage, network load, and system performance**.

Deployed as part of the **GreenDIGIT WP6.2** research activities, this module integrates with:

- [WP6.1 Environmental Metric Publication System](#)
- [WP6.3 Energy-Aware Brokering Framework](#)
- UTH real-time IoT metrics infrastructure, data and workloads
- SoBigData RI metrics ecosystem
- IFCA and DIRAC records infrastructure

### To-dos (create tickets)
- [ ] DVC assets imported from remote storage (GDrive, AWS or server)
- ML model is quite simple. Things to improve.
  - [x] XGBoost, CatBoost (or other SoTA gradient boost tool-algo)
  - [x] Deep Learning: Convolutional Neural Network (LSTM, Temporal Convolution, Transformer) with PyTorch or TensorFlow
  - [ ] Use `scikit-learn-onnx` for more adaptability to edge-devices
  - [x] Integrate MQTT and/or Prometheus for edge-optimised messaging telemetry between devices (for the Edge)
- [ ] Metrics' ingestion: batch API from CNR
- [x] Metrics' ingestion: Kafka + MQTT + Flink/Spark + PostgreSQL/InfluxDB
- [ ] (Optional) Testbed implementation IoT with UTH

## Models used

### Baseline: HistGradientBoostingRegressor (HGBR)
A tree-boosting model from `scikit-learn` used as the initial baseline. It operates on tabular, engineered features (lags, rolling statistics), is fast to train, handles missing values with an imputer, and gives a strong reference MAE/RMSE to beat.

### XGBoost (Gradient Boosted Trees)
High-performance gradient boosting on decision trees (histogram algorithm). Strong on heterogeneous tabular data, captures non-linear interactions well, robust to missing values, and typically competitive as a production baseline. In our pipeline it reads the same engineered features as the baseline and logs train/validation/test metrics plus validation curves to DVC.


### LSTM (sequence model)
A recurrent neural network that ingests sliding **time windows** shaped as `[samples, timesteps, features]`, therefore modelling temporal dependencies explicitly. Useful when recent history strongly determines near-future power. Requires normalised inputs and careful tuning; CPU training is slower than tree models.

### ARIMA/SARIMA
(Seasonal/) Autoregression Integrated Moving Average.
- [ ] TODO: write description.

> The pipeline includes a **champion selector** which chooses the best model by test error and writes `models/champion.json` for the inference service to load.

## Running the service and getting predictions

### 1) Train models and pick a winner
Ensure the DVC pipeline has produced models and metrics:
```bash
# From repo root
dvc repro          # runs ingest ‚Üí featurise ‚Üí train ‚Üí validate_features ‚Üí train_xgb ‚Üí reshape_windows ‚Üí train_lstm ‚Üí select_champion
dvc metrics show   # compare baseline / xgb / lstm / (s)arima
```

### 2) Start the FastAPI service
```bash
uvicorn service.main:app --host 0.0.0.0 --port 8000
```

Health and model info:
```bash
curl http://localhost:8000/health
curl http://localhost:8000/model
```

### 3) Request a prediction
If the champion is **XGBoost/HGBR (tabular)**, send a flat feature dict.
```bash
curl -X POST http://localhost:8000/predict \
  -H 'Content-Type: application/json' \
  -d '{
        "features": {
          "cpu_usage_percent": 12.3,
          "memory_used_bytes": 8200000000,
          "network_bw_rx_b/s": 155000,
          "lag_1h": 320.5,
          "lag_2h": 315.1,
          "lag_3h": 318.9,
          "lag_6h": 310.2
        }
      }'
# ‚Üí {"power_forecast": <number>}
```
If the "champion" is **LSTM (sequence)**, send the most recent window `[timesteps]` `[features]`.
```bash
curl -X POST http://localhost:8000/predict \
  -H 'Content-Type: application/json' \
  -d '{
        "window": [
          [12.3, 8200000000, 155000, ...],
          [13.1, 8300000000, 160000, ...],
          [11.9, 8100000000, 150000, ...]
          // ... up to the configured window length (e.g., 60 steps)
        ]
      }'
# ‚Üí {"power_forecast": <number>}
```

#### Notes
- The service loads `models/champion.json`, so deployment remains model-agnostic.
- If your features use CIM paths or aliases, map them to the canonical na
mes server-side before padding missing inputs.
- For production, persist any scalers with the model and apply them inside the service, validate inputs, and secure the endpoint.

---

## Metrics Ingestion Services (Features)
- Ingest, Featurise and Train stages in-built as a pipeline (with DVC tracking).
- FastAPI server `/predict` endpoint with a `{"power_forecast":<number>}` result.
- MQTT + Kafka + Flink streaming pipeline

### MQTT + Kafka + Flink pipeline tutorial (development)
1. Install `docker-compose` with all containerised services (MQTT + Kafka).
```bash
cd streaming_service # you should see a docker-compose.yaml if you run ls -la
docker compose up -d --build
```

This will spin-up several services included in the compose file, including Kafka-UI, MQTT broker/subscriber and a Kafka bridge that ingests that service.
To see the logs from MQTT and Kafka respectively:
- `docker logs -f mqtt`
- `docker logs -f mqtt_to_kafka`

2. To start the synthetic workloads
```bash
# Go to the synthetic metrics' workload folder.
cd synthetic_metrics_service

# If you do not have the environment installed.
python -m venv .
source bin/activate

# Inside our environment:
python metrics_publisher.py
# From here you should see metrics being recurrently logged.
```

3. Kafka -> ELT (Flink SQL) -> Iceberg + MinIO
```sh
# Some useful command to list Kafka's topics, for debugging.
docker exec -it kafka kafka-topics.sh --bootstrap-server localhost:9092 --list

docker exec -it kafka /opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list
docker exec -it kafka /opt/bitnami/kafka/bin/kafka-configs.sh --bootstrap-server localhost:9092 \
  --entity-type topics --entity-name metrics.raw.stream --describe

```

4. Generating metrics (temporary)
```sh
# 1) Generate namespaces.json
python generate_namespaces.py --n 12

# 2a) Use existing namespace.json (no auto-generate nodes) (defaults SourceType=IoT)
python generate_synthetic_metrics.py --days 1 --freq-mins 3

# 2b) (Optional) autogenerate 12 IoT nodes
python generate_synthetic_metrics.py --autogen-nodes 12 --days 1 --freq-mins 3

# 3) Publish metrics
# 3-second fixed cadence, override payload timestamps to "now", IoT only
PACE_MODE=cadence CADENCE_S=3 OVERRIDE_TS=true SOURCE_TYPE=IoT \
BROKER=localhost PORT=1883 TOPIC_ROOT=greendigit QOS=1 \
python metrics_publisher.py

# Or: respect recorded Œîts (scaled), keep original timestamps
PACE_MODE=replay_ts REPLAY_SPEED=2.0 OVERRIDE_TS=false SOURCE_TYPE=IoT \
python metrics_publisher.py

```

---

## Architecture
### Overview Architecture
![Overview Architecture](assets/gd_ecomep_overview_architecture.png)
- [ ] TODO: write description.

### Data Flow Architecture
![ECoMEP Data Flow](assets/gd_ecomep_pipeline.png)
- [ ] TODO: write description.

## Machine Learning Pipeline

### Ingestion & Preprocessing
- Collect metrics from edge nodes, sensors, and cluster logs
- Use **MQTT**, **Prometheus**, or **Kafka/NATS**
- Normalie, timestamp-align, and validate data

### Model Training
- Train using:
  - **Time Series Forecasting** (LSTM, Prophet)
  - **Regression/Classification** (XGBoost, RF)
  - **Energy/Latency Prediction**
- Tools: **PyTorch**, **TensorFlow**, **Scikit-learn**

### Real-Time Inference
- ONNX or TensorFlow Lite models served at edge
- Model registry: MLflow or DVC-based

---

## Folder Structure
- `.dvc/`, `dvc.yaml`, `dvc.lock` ‚Äî DVC pipeline and metadata.
- `data/` ‚Äî raw/clean/features/windows datasets managed by DVC.
- `metrics/` ‚Äî JSON metrics tracked by DVC (baseline, XGBoost, LSTM).
- `models/` ‚Äî trained artefacts (`baseline.joblib`, `xgb.joblib`, `lstm.pt`, `champion.json`).
- `scripts/` ‚Äî pipeline scripts (`ingest.py`, `featurise.py`, `train.py`, `train_xgb.py`, `make_windows.py`, `train_lstm.py`, etc.).
- `service/` ‚Äî FastAPI inference service (model-agnostic champion loader).
- `synthetic_metrics_service/`, `streaming_service/`, `ingest_service/` ‚Äî data generation and streaming/ELT components.
- `assets/` ‚Äî documentation assets.

<!-- ```bash
.
‚îú‚îÄ‚îÄ ingestion/             # Metric ingestion and connectors
‚îú‚îÄ‚îÄ preprocessing/         # Data cleaning and transformation
‚îú‚îÄ‚îÄ training/              # Training scripts and model tracking
‚îú‚îÄ‚îÄ inference/             # Model serving scripts (ONNX, Lite)
‚îú‚îÄ‚îÄ deployment/            # Helm charts, Dockerfiles
‚îú‚îÄ‚îÄ crate/                 # RO-Crate metadata, licences, schema
‚îú‚îÄ‚îÄ ro-crate-metadata.json
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
``` -->

## Outputs and Publications
Unified JSON or RO-Crate formatted metrics

- `/FETCH` endpoint compatible with WP6.1 publication system
- Optionally `POST`ed to:
    - cASO and Grid record services
    - CIM record registry with auth token

### Interoperability
- RO-Crate compliant for FAIR metadata
- Containerised for deployment in federated clusters
- Compatible with SoBigData metrics registry and Dirac grid APIs
- Modular, with pluggable ML models and data formats

## Citation
```
@software{GreenDIGIT_WP62,
  title = {Edge-Cloud Continuum Multi-Level Predictive Framework},
  author = {GreenDIGIT WP6.2 Contributors},
  year = {2025},
  version = {v1.0},
  url = {https://github.com/GreenDIGIT/WP6.2-Predictive-Framework}
}
```

## Contributors
Gon√ßalo Ferreira ‚Äì UvA Researcher - WP6.2 Developer
- [ ] [Collaborators, Partners]

Supported by GreenDIGIT, SoBigData RI, IFCA, DIRAC, and CNR.

## Contact
For questions, integration requests or metric schema definitions, contact:

GreenDIGIT WP6.2 Team
üìß contact@greendigit.eu
üåê greendigit.eu

